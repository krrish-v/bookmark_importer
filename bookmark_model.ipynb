{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hST5IXXj2LXT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "N8yhl-tG26TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "aWXCFZ8w38ZW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "t1BA2rZKdU13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from absl import logging\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "from absl import logging\n",
        "import pandas as pd\n",
        "\n",
        "class Use_model:\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        THis uses the lite version of universal sentence encoder\n",
        "            ~ https://tfhub.dev/google/universal-sentence-encoder-lite/2\n",
        "\n",
        "        Also see here: https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite\n",
        "        '''\n",
        "\n",
        "        spm_path = \"https://tfhub.dev/google/universal-sentence-encoder-lite/2\"\n",
        "\n",
        "        module = hub.Module(spm_path)\n",
        "\n",
        "        self.input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\n",
        "        self.encodings = module(\n",
        "            inputs=dict(\n",
        "                values=self.input_placeholder.values,\n",
        "                indices=self.input_placeholder.indices,\n",
        "                dense_shape=self.input_placeholder.dense_shape))\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            spm_path = sess.run(module(signature=\"spm_path\"))\n",
        "\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        \n",
        "        with tf.io.gfile.GFile(spm_path, mode=\"rb\") as f:\n",
        "            self.sp.LoadFromSerializedProto(f.read())\n",
        "        \n",
        "        print(\"SentencePiece model loaded at {}.\".format(spm_path))\n",
        "\n",
        "    def process_to_IDs_in_sparse_format(self, sp, sentences):\n",
        "\n",
        "        '''\n",
        "         An utility method that processes sentences with the sentence piece processor\n",
        "         'sp' and returns the results in tf.SparseTensor-similar format:\n",
        "         (values, indices, dense_shape)\n",
        "        '''\n",
        "        ids = [sp.EncodeAsIds(x) for x in sentences]\n",
        "        max_len = max(len(x) for x in ids)\n",
        "        dense_shape=(len(ids), max_len)\n",
        "        values=[item for sublist in ids for item in sublist]\n",
        "        indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
        "        return (values, indices, dense_shape)\n",
        "\n",
        "    def text_vector(self, sentence):\n",
        "\n",
        "        self.sentence = sentence\n",
        "        messages = [self.sentence]\n",
        "\n",
        "        values, indices, dense_shape = self.process_to_IDs_in_sparse_format(self.sp, messages)\n",
        "\n",
        "        # Reduce logging output.\n",
        "        logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "        input_placeholder = self.input_placeholder\n",
        "\n",
        "        with tf.Session() as session:\n",
        "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "            message_embeddings = session.run(\n",
        "                self.encodings,\n",
        "                feed_dict={input_placeholder.values: values,\n",
        "                            input_placeholder.indices: indices,\n",
        "                            input_placeholder.dense_shape: dense_shape})\n",
        "\n",
        "        self.vector = np.array(message_embeddings).tolist()\n",
        "\n",
        "        return self.vector\n"
      ],
      "metadata": {
        "id": "Hzb2xlhvX3Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = Use_model()"
      ],
      "metadata": {
        "id": "7BXzfgYXa5NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.text_vector('I love this man')"
      ],
      "metadata": {
        "id": "j9yRfKPZJa2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/medium_articles_drop.csv')\n",
        "data.drop(['authors', 'timestamp'], inplace=True, axis=1)\n",
        "\n",
        "data.head"
      ],
      "metadata": {
        "id": "R3UnfXvo-wBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = data['tags']\n",
        "title = data['text']\n",
        "heading = data['title']"
      ],
      "metadata": {
        "id": "Dyl2up9H-5vj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tags = np.array(tags)\n",
        "all_tags.shape"
      ],
      "metadata": {
        "id": "1XsFMO6QoyPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8caccb-de21-4bcd-fb17-516d195c07b3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1927,)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "uCFUDbY8FGJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "from nltk import SnowballStemmer, WordNetLemmatizer, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import re\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "3-R_V1awO82e"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "tMx51FGsY8oG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tokens = []\n",
        "\n",
        "# textcleaning\n",
        "def long_text_cleaning(text: list):\n",
        "     \n",
        "    for sen in text:\n",
        "        sentence_tokenize = sent_tokenize(sen)\n",
        "\n",
        "        cleaned_sen = []\n",
        "        for sent_token in sentence_tokenize:\n",
        "            \n",
        "            new_sen = []\n",
        "\n",
        "            words_tokenize = word_tokenize(sent_token)\n",
        "            prasser_tags = pos_tag(words_tokenize)\n",
        "            \n",
        "            for token, tag in prasser_tags:\n",
        "                token = re.sub(\"[!$%&'()*+,-./:;<=>?@[\\]^_`{|} ~0-9]\",\"\", token)\n",
        "\n",
        "                if len(token) != 1:\n",
        "\n",
        "                    if tag.startswith(\"NN\"):\n",
        "                        pos = 'n'\n",
        "                    elif tag.startswith('VB'):\n",
        "                        pos = 'v'\n",
        "                    else:\n",
        "                        pos = 'a'\n",
        "\n",
        "                    lemmatizer = WordNetLemmatizer()\n",
        "                    token = lemmatizer.lemmatize(token, pos)\n",
        "            \n",
        "                    filtered_word = token.lower()\n",
        "\n",
        "                    if len(token) > 0 and filtered_word not in stop_words:\n",
        "                        new_sen.append(filtered_word)\n",
        "\n",
        "            cleaned_sen.append([' '.join(new_sen)])\n",
        "\n",
        "        cleaned_tokens.append(cleaned_sen)\n",
        "\n",
        "long_text_cleaning(title)\n"
      ],
      "metadata": {
        "id": "rJXogbXgNJ6L"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = [' '.join(sent_[0] for sent_ in sent) for sent in cleaned_tokens]"
      ],
      "metadata": {
        "id": "nerVNI642YAq"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text[0]"
      ],
      "metadata": {
        "id": "l7MPuj7PQZvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### transform text to vector "
      ],
      "metadata": {
        "id": "EX2WmoLRrZg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "use_title = []\n",
        "\n",
        "count = 0\n",
        "for text_ in cleaned_text:\n",
        "  print(count)\n",
        "  count+=1\n",
        "  use_title.append(embedding.text_vector(text_))\n"
      ],
      "metadata": {
        "id": "XI9j0edKWEEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### It roughly takes 3.5 hours to get the embeddings"
      ],
      "metadata": {
        "id": "Aiy5QE7JoP8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_title[0]"
      ],
      "metadata": {
        "id": "y117aGZbRIXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(use_title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1ZajcJ7Mj36",
        "outputId": "52473179-50fd-475f-e793-5ae861e76f88"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1927"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "7r69Ds_7p6Nb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/drive/MyDrive/use_title.json', 'w')\n",
        "\n",
        "try:\n",
        "  json.dump(use_title, f)\n",
        "except:\n",
        "  json.dumps(use_title, f)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "ybU0RY4FejYr"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reforming heading"
      ],
      "metadata": {
        "id": "V0HcVk5wRa4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def heading_cleaning(title: np.array):\n",
        "    \n",
        "    all_cleaned_title = []\n",
        "    stemmer = SnowballStemmer('english')\n",
        "\n",
        "    for tit in title:\n",
        "        cleaned_title = []\n",
        "        all_tit_words = word_tokenize(tit)\n",
        "\n",
        "        for word in all_tit_words:\n",
        "\n",
        "            word = re.sub('[0-9]', '', word)\n",
        "            word = stemmer.stem(word)\n",
        "        \n",
        "            if word not in stop_words and word not in punctuation and len(word) > 1:\n",
        "\n",
        "                cleaned_title.append(word)\n",
        "        \n",
        "        all_cleaned_title.append(' '.join(cleaned_title))\n",
        "    \n",
        "    return all_cleaned_title\n",
        "\n",
        "all_cleaned_title = heading_cleaning(heading)"
      ],
      "metadata": {
        "id": "UGvnJTdZEeov"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_cleaned_title[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "brOPxBSPExri",
        "outputId": "022761c5-d7f5-481d-ea05-b300edc84c28"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mental note vol'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tit = np.array(all_cleaned_title)\n",
        "\n",
        "cleaned_tit.shape"
      ],
      "metadata": {
        "id": "GETp4jC6zMlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ad8595-dfc0-4c79-f2a2-d5616b277dc6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1927,)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_ = open('/content/drive/MyDrive/use_title.json', 'r')\n",
        "\n",
        "use_title = json.load(f_)\n",
        "f_.close()\n",
        "len(use_title)"
      ],
      "metadata": {
        "id": "EH4R4qgboHr_",
        "outputId": "9a3bb601-4f7f-4ad8-aec7-7d983d55cfa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1927"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "use_title = np.array(use_title).reshape(1927, -512)\n",
        "use_title.dtype, use_title.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaCQQIG1f_Dw",
        "outputId": "2cef6511-facb-4f4f-b5e5-bb3b4d294aa8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dtype('float64'), (1927, 512))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_title[0].shape"
      ],
      "metadata": {
        "id": "R9FplkSzlf6j",
        "outputId": "b193a759-9e73-4794-efe7-240ebc0ea618",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512,)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "x8L9h-JpE2tW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On K-means"
      ],
      "metadata": {
        "id": "E6NNkO_tsXYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_clusters = 600\n",
        "means = KMeans(n_clusters=no_of_clusters)\n",
        "means.fit(use_title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehkcbrJexRDD",
        "outputId": "3f50ecbd-a3c2-4b11-854e-b33e1ffcc36f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(n_clusters=600)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_model = open('prid_model.sav', 'wb')\n",
        "pickle.dump(means, f_model)\n",
        "f_model.close()"
      ],
      "metadata": {
        "id": "OqIlfVaecG0h"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_km = means.fit_predict(use_title)"
      ],
      "metadata": {
        "id": "JhBqggSKy0l4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = cleaned_tit\n",
        "\n",
        "for i in range(no_of_clusters):\n",
        "    y_plot = use_title[y_km==i, 0]\n",
        "    x_plot = np.array([y[np.where(use_title == ind)[0][0]] for ind in y_plot]).reshape(-1)\n",
        "\n",
        "    plt.scatter(x_plot, y_plot, s=5)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "apUdhYEUYiFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_tags = {}\n",
        "\n",
        "for i in range(no_of_clusters):\n",
        "    \n",
        "    k1 = use_title[y_km==i, :512]\n",
        "    topic = np.array([all_tags[np.where(use_title == ind)[0][0]] for ind in k1]).reshape(-1)\n",
        "\n",
        "    noth_topic = []\n",
        "    for w in topic:\n",
        "      \n",
        "      seno_topic = word_tokenize(re.sub(\"[!$%&'()*+,-./:;<=>?@[\\]^_`{|}~0-9]\", '', w))\n",
        "      \n",
        "      ray_topic = [topc for topc in seno_topic if len(topc) > 2]\n",
        "      noth_topic.append(ray_topic)\n",
        "\n",
        "    set_tags['topic ' + str(i)] = noth_topic\n"
      ],
      "metadata": {
        "id": "XbYAJQmvVBzO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_tags['topic 3']"
      ],
      "metadata": {
        "id": "o4hyIWOyq_I8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ca90be-1731-478b-d02c-a272b51d4b2c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Neuroscience', 'Health', 'Science', 'Psychology'],\n",
              " ['Health', 'Covid', 'Body', 'Coronavirus', 'Science'],\n",
              " ['Technology', 'Health', 'Covid', 'Life', 'Science'],\n",
              " ['Selfawareness', 'Body', 'Safety', 'Health', 'Empowerment']]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "f_prid_tags = open('pridicted_tags.json', 'w')\n",
        "json.dump(set_tags, f_prid_tags)\n",
        "f_prid_tags.close()"
      ],
      "metadata": {
        "id": "J9tiA9fRTpzb"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}